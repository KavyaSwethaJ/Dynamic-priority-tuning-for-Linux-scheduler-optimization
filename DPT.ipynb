{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "sudo visudo\n",
        "<user> ALL=(ALL) NOPASSWD: ALL"
      ],
      "metadata": {
        "id": "k_JooO3RDovZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy0enWk1dPjr",
        "outputId": "742dab51-31ad-4a28-b28d-5c47ae2469b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 'docker-init', 0, '0', '59', '6', 0.0, 6.0176328678293134e-05, 3391, 2499, '1', 'sleeping')\n",
            "(6, 'node', 1, '0', '170', '30', 0.0, 0.36572163754232656, 9909, 3735, '11', 'sleeping')\n",
            "(18, 'tail', 0, '0', '64', '8', 0.0, 0.004362783829176252, 57, 11, '1', 'sleeping')\n",
            "(28, 'oom_monitor.sh', 6, '0', '17', '56', 0.0, 0.025274058044883113, 35511, 3801, '1', 'sleeping')\n",
            "(30, 'run.sh', 1, '0', '0', '0', 0.0, 0.01251667636508497, 0, 0, '1', 'sleeping')\n",
            "(33, 'kernel_manager_proxy', 30, '0', '21', '13', 0.0, 0.1029917865328987, 1172, 761, '5', 'sleeping')\n",
            "(59, 'python3', 6, '0', '698', '117', 0.0, 0.0, 8741, 3, '1', 'zombie')\n",
            "(60, 'colab-fileshim.', 6, '0', '76', '13', 0.0, 0.3258247316286182, 1299, 1, '1', 'sleeping')\n",
            "(77, 'jupyter-noteboo', 6, '0', '434', '66', 0.0, 1.1554456869519065, 5014, 572, '7', 'sleeping')\n",
            "(78, 'dap_multiplexer', 6, '0', '28', '12', 0.0, 0.0770858770368935, 1397, 1319, '5', 'sleeping')\n",
            "(443, 'python3', 77, '0', '547', '76', 0.0, 0.7033710177562293, 4226, 525, '14', 'running')\n",
            "(518, 'python3', 1, '0', '43', '7', 0.0, 0.10446610658551689, 4, 0, '7', 'sleeping')\n",
            "(2758, 'language_service', 6, '0', '5', '3', 0.0, 0.10879880225035399, 194, 117, '7', 'sleeping')\n",
            "(2763, 'node', 2758, '0', '1559', '143', 0.0, 1.7609399061128919, 1876, 1443, '8', 'sleeping')\n",
            "(5238, 'sleep', 28, '0', '0', '0', 0.0, 0.003911461364089054, 12, 0, '1', 'sleeping')\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import psutil\n",
        "\n",
        "data = []\n",
        "for i in psutil.pids():\n",
        "    try:\n",
        "        p = psutil.Process(i)\n",
        "        res = os.popen(f\"more /proc/{i}/stat\").read()\n",
        "        res = res.split()\n",
        "        data.append(\n",
        "            (\n",
        "                p.pid, p.name(),\n",
        "                p.ppid(), res[21],\n",
        "                res[16],\n",
        "                res[17],\n",
        "                p.cpu_percent(),\n",
        "                p.memory_percent(),\n",
        "                p.io_counters().read_count,\n",
        "                p.io_counters().write_count,\n",
        "                res[22],\n",
        "                p.status(),\n",
        "            )\n",
        "        )\n",
        "    except:\n",
        "        continue\n",
        "for i in data:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhuDyCV_hJ1W",
        "outputId": "e299aa3a-b788-406c-c50c-07271f7bc4d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m91\u001b[0m\n\u001b[0;31m    new_q = current_q + alpha * (reward + gamma * max_future_q - current_q)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Define the state space\n",
        "def get_state():\n",
        "    data = []\n",
        "    for i in psutil.pids():\n",
        "        try:\n",
        "            p = psutil.Process(i)\n",
        "            res = os.popen(f\"more /proc/{i}/stat\").read()\n",
        "            res = res.split()\n",
        "            data.append(\n",
        "                (\n",
        "                    p.pid,\n",
        "                    p.name(),\n",
        "                    p.username(),\n",
        "                    res[20],\n",
        "                    res[21],\n",
        "                    psutil.cpu_percent(),\n",
        "                    psutil.virtual_memory().percent,\n",
        "                    psutil.disk_usage(\"/\").percent,\n",
        "                )\n",
        "            )\n",
        "        except:\n",
        "            continue\n",
        "    return data\n",
        "\n",
        "# Set the reward value\n",
        "reward = 1\n",
        "\n",
        "# Define the Q-table\n",
        "q_table = {}\n",
        "try:\n",
        "    with open(\"q_table.pkl\", \"rb\") as f:\n",
        "        q_table = pickle.load(f)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Define the hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "# Define the action space\n",
        "action_space = [-20, -10, -5, 0, 5, 10, 20]\n",
        "\n",
        "\n",
        "# Initialize the state\n",
        "\n",
        "# Define the epsilon-greedy policy\n",
        "def epsilon_greedy_policy(state, epsilon):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return random.choice(action_space)\n",
        "    else:\n",
        "        if state in q_table:\n",
        "            return max(q_table[state],\n",
        "                       key=q_table[state].get)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    # while(True):\n",
        "    state_space = get_state()\n",
        "    # Train the Q-learning algorithm\n",
        "    for j in range(len(state_space)):\n",
        "        # Set the initial priority\n",
        "        priority = state_space[j][4]\n",
        "\n",
        "        # Choose an action based on the epsilon-greedy policy\n",
        "        action = epsilon_greedy_policy(state_space[j][1::], epsilon)\n",
        "        try:\n",
        "            # Apply the action\n",
        "            priority += action\n",
        "            psutil.Process(state_space[j][0]).nice(priority)\n",
        "\n",
        "            # Observe the new state\n",
        "            new_state = state_space[j + 1][1::]\n",
        "\n",
        "            # Get the current Q-value\n",
        "            current_q = q_table.get((state_space[j][1::], action), 0)\n",
        "\n",
        "            # Get the max Q-value for the new state\n",
        "            max_future_q = max(q_table.get(new_state, {0: 0}).values())\n",
        "\n",
        "      # Calculate the new Q-value\n",
        "      new_q = current_q + alpha * (reward + gamma * max_future_q - current_q)\n",
        "\n",
        "      # Update the Q-table\n",
        "      q_table[(state_space[j][1::], action)] = new_q\n",
        "\n",
        "      # Update the state\n",
        "      # state_space[j][1::] = new_state\n",
        "\n",
        "      # Wait for some time\n",
        "      time.sleep(0.5)\n",
        "      except:\n",
        "          pass\n",
        "      # Save the Q-table\n",
        "      with open(\"q_table.pkl\", \"wb\") as f:\n",
        "          pickle.dump(q_table, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import psutil\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Define the state space\n",
        "def get_state():\n",
        "    data = []\n",
        "    for i in psutil.pids():\n",
        "        try:\n",
        "            p = psutil.Process(i)\n",
        "            res = os.popen(f\"more /proc/{i}/stat\").read()\n",
        "            res = res.split()\n",
        "            data.append(\n",
        "                (\n",
        "                    p.pid,\n",
        "                    p.name(),\n",
        "                    p.username(),\n",
        "                    int(res[20]),\n",
        "                    int(res[21]),\n",
        "                    psutil.cpu_percent(),\n",
        "                    psutil.virtual_memory().percent,\n",
        "                    psutil.disk_usage(\"/\").percent,\n",
        "                )\n",
        "            )\n",
        "        except:\n",
        "            continue\n",
        "    return data\n",
        "\n",
        "# Set the reward and penalty values\n",
        "reward = 1\n",
        "penalty = -1\n",
        "\n",
        "# Define the Q-table\n",
        "q_table = {}\n",
        "try:\n",
        "    with open(\"q_table.pkl\", \"rb\") as f:\n",
        "        q_table = pickle.load(f)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Define the hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.3\n",
        "\n",
        "# Define the action space\n",
        "# action_space = [-20, -10, -5, 0, 5, 10, 20]\n",
        "action_space = [0 , 1]\n",
        "state_space = []\n",
        "j = 0\n",
        "\n",
        "# Initialize the state\n",
        "\n",
        "# Define the epsilon-greedy policy\n",
        "def epsilon_greedy_policy(state, epsilon):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return random.choice(action_space)\n",
        "    else:\n",
        "        if state in q_table:\n",
        "            return max(q_table[state], key=q_table[state].get)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    # while(True):\n",
        "    state_space = get_state()\n",
        "    # Train the Q-learning algorithm\n",
        "    for j in range(len(state_space)):\n",
        "        # Set the initial priority\n",
        "        priority = state_space[j][4]\n",
        "\n",
        "        # Choose an action based on the epsilon-greedy policy\n",
        "        action = epsilon_greedy_policy(state_space[j][1::], epsilon)\n",
        "\n",
        "        # Apply the action\n",
        "        # priority += action\n",
        "        priority -= action\n",
        "        print(f\"Priority = {priority}\")\n",
        "        try:\n",
        "            psutil.Process(state_space[j][0]).nice(priority)\n",
        "        except:\n",
        "            continue\n",
        "        try:\n",
        "            # Observe the new state\n",
        "            new_state = state_space[j + 1][1::]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Get the current Q-value\n",
        "        current_q = q_table.get((state_space[j][1::], action), 0)\n",
        "\n",
        "        # Get the max Q-value for the new state\n",
        "        max_future_q = max(q_table.get(new_state, {0: 0}).values())\n",
        "\n",
        "        # Calculate the new Q-value\n",
        "        new_q = current_q + alpha * (reward + gamma * max_future_q - current_q)\n",
        "\n",
        "        # Update the Q-table\n",
        "        q_table[(state_space[j][1::], action)] = new_q\n",
        "\n",
        "        # Update the state\n",
        "        # state_space[j][1::] = new_state\n",
        "\n",
        "        # Wait for some time\n",
        "        time.sleep(0.01)\n",
        "\n",
        "        # Save the Q-table\n",
        "        with open(\"q_table.pkl\", \"wb\") as f:\n",
        "            pickle.dump(q_table, f)\n"
      ],
      "metadata": {
        "id": "uJwtXZITvM0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Define the state space\n",
        "def get_state():\n",
        "    data = []\n",
        "    for i in psutil.pids():\n",
        "        try:\n",
        "            p = psutil.Process(i)\n",
        "            res = os.popen(f\"more /proc/{i}/stat\").read()\n",
        "            res = res.split()\n",
        "            data.append(\n",
        "                (\n",
        "                    p.pid,\n",
        "                    p.ppid,\n",
        "                    p.name(),\n",
        "                    int(res[20]),\n",
        "                    int(res[21]),\n",
        "                    psutil.cpu_percent(),\n",
        "                    psutil.virtual_memory().percent\n",
        "                )\n",
        "            )\n",
        "            psutil.cpu_percent(),\n",
        "            psutil.virtual_memory().percent\n",
        "        except:\n",
        "            continue\n",
        "    return data\n",
        "\n",
        "# Set the reward and penalty values\n",
        "reward = 1\n",
        "penalty = -1\n",
        "\n",
        "# Define the Q-table\n",
        "q_table = {}\n",
        "try:\n",
        "    with open(\"q_table.pkl\", \"rb\") as f:\n",
        "        q_table = pickle.load(f)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Define the hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "# Define the action space\n",
        "# action_space = [-20, -10, -5, 0, 5, 10, 19]\n",
        "action_space = [0, 1]\n",
        "state_space = []\n",
        "j = 0\n",
        "\n",
        "# Initialize the state\n",
        "\n",
        "def check_state(state, q_table):\n",
        "  for key in q_table.keys():\n",
        "      if key[0] == state:\n",
        "          return True\n",
        "  return False\n",
        "\n",
        "# Define the epsilon-greedy policy\n",
        "def epsilon_greedy_policy(state, epsilon):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return random.choice(action_space)\n",
        "    else:\n",
        "        if check_state(state,q_table):\n",
        "            return max(q_table[state], key=q_table[state].get)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "while(True):\n",
        "    state_space = get_state()\n",
        "    # Train the Q-learning algorithm\n",
        "    for j in range(len(state_space)):\n",
        "        if ((state_space[j][0] != 1) or (state_space[j][1] != 1)):\n",
        "            # Set the initial priority\n",
        "            priority = state_space[j][4]\n",
        "\n",
        "            # Choose an action based on the epsilon-greedy policy\n",
        "            action = epsilon_greedy_policy(state_space[j][2::], epsilon)\n",
        "\n",
        "            # Apply the action\n",
        "            priority -= action\n",
        "            try:\n",
        "                #psutil.Process(state_space[j][0]).nice(priority)\n",
        "                os.popen(f\"sudo renice -n {priority} -p {state_space[j][0]}\")\n",
        "            except:\n",
        "                pass\n",
        "            try:\n",
        "                # Observe the new state\n",
        "                new_state = state_space[j + 1][2::]\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # Get the current Q-value\n",
        "            current_q = q_table.get((state_space[j][2::], action), 0)\n",
        "\n",
        "            # Get the max Q-value for the new state\n",
        "            max_future_q = max(q_table.get(new_state, {0: 0}).values())\n",
        "\n",
        "            # Calculate the new Q-value\n",
        "            new_q = current_q + alpha * (reward + gamma * max_future_q - current_q)\n",
        "\n",
        "            # Update the Q-table\n",
        "            q_table[(state_space[j][2::], action)] = new_q\n",
        "\n",
        "            # Update the state\n",
        "            # state_space[j][2::] = new_state\n",
        "\n",
        "    # Wait for some time\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    # Save the Q-table\n",
        "    with open(\"q_table.pkl\", \"wb\") as f:\n",
        "        pickle.dump(q_table, f)"
      ],
      "metadata": {
        "id": "8lEGdGUmul3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_state(('python3', 39, 19), q_table)"
      ],
      "metadata": {
        "id": "opcGemkz499i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(q_table[('python3', 39, 19)], key=q_table[('python3', 39, 19)].get)"
      ],
      "metadata": {
        "id": "XLoAy0N-5VFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_table[state_space[0][1::]]"
      ],
      "metadata": {
        "id": "n6CCKvJazs1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in q_table:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "rqVjNeMSyZme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_table[('python3', 39, 19)]"
      ],
      "metadata": {
        "id": "Kkp-JiyXeSDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(state_space)"
      ],
      "metadata": {
        "id": "26pvHxJrvtWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('q_table.pickle', 'rb') as f:\n",
        "        q_table = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "gVQQhp68tG9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-Y9ndLYzUkD"
      },
      "outputs": [],
      "source": [
        "print(q_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMM9XGoh5Z_p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import tensorflow\n",
        "# from tensorflow.keras.layers import Dense\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "EPISODES = 100\n",
        "MAX_TIMESTEPS = 100\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95  # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = tensorflow.keras.models.Sequential()\n",
        "        model.add(tensorflow.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(tensorflow.keras.layers.Dense(24, activation='relu'))\n",
        "        model.add(tensorflow.keras.layers.Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=tensorflow.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    state_size = 3  # number of state variables to track\n",
        "    action_size = 5  # number of possible priority values\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "    # user behavior data collection and preprocessing\n",
        "    # create state variables based on user behavior data\n",
        "    # normalize state variables (if necessary)\n",
        "\n",
        "    # training loop\n",
        "    for e in range(EPISODES):\n",
        "        # reset environment and initialize state variables\n",
        "        state = (psutil.virtual_memory().percent, psutil.cpu_percent())\n",
        "        # time steps within each episode\n",
        "        for time in range(MAX_TIMESTEPS):\n",
        "            # choose action\n",
        "            action = agent.act(state)\n",
        "\n",
        "            # perform action and get reward\n",
        "            reward = 1\n",
        "\n",
        "            # observe new state and store in replay buffer\n",
        "            next_state = (psutil.virtual_memory().percent, psutil.cpu_percent())\n",
        "            agent.remember(state, action, reward, next_state, True)\n",
        "\n",
        "            # update state\n",
        "            state = next_state\n",
        "\n",
        "            # perform replay and target model updates\n",
        "            if len(agent.memory) > BATCH_SIZE:\n",
        "                agent.replay(BATCH_SIZE)\n",
        "\n",
        "        # save model weights\n",
        "        if e % 10 == 0:\n",
        "            agent.save(\"model_\" + str(e) + \".h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "from time import sleep\n",
        "cpu = []\n",
        "mem = []\n",
        "for i in range(120):\n",
        "  cpu.append(psutil.cpu_percent())\n",
        "  mem.append(psutil.virtual_memory().percent)\n",
        "  sleep(0.5)\n",
        "print(cpu)\n",
        "print(mem)"
      ],
      "metadata": {
        "id": "xmVWcYH60HA9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c76d69-ce03-42b6-ab12-82d9bf7792fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12.9, 2.0, 3.0, 2.0, 3.0, 3.0, 3.0, 0.0, 4.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 3.0, 46.5, 56.4, 52.5, 29.0, 1.0, 4.0, 5.1, 2.0, 1.0, 3.0, 1.0, 2.0, 2.0, 5.0, 0.0, 4.0, 3.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 35.0, 51.5, 53.5, 40.0, 5.1, 2.0, 2.0, 1.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0, 3.0, 3.0, 2.0, 2.0, 2.0, 5.0, 3.0, 1.0, 1.0, 22.0, 54.0, 52.5, 55.0, 2.0, 3.0, 1.0, 3.0, 0.0, 3.0, 0.0, 3.0, 1.0, 3.0, 2.0, 1.0, 2.9, 2.0, 1.0, 2.0, 4.0, 2.0, 2.0, 6.1, 58.0, 51.5, 56.6, 12.0, 6.0, 0.0, 2.0, 1.0, 3.0, 0.0, 5.0, 56.0, 53.0, 3.0, 5.1, 3.0, 2.0, 2.0, 2.0, 0.0, 3.0, 5.0, 4.0, 46.5, 57.0, 54.5, 31.0, 2.0, 2.0, 0.0, 4.0, 0.0, 7.9, 0.0]\n",
            "[7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.7, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.7, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8, 7.8]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}